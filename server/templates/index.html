<!DOCTYPE html>
<html>
    <head></head>
    <style>
        body {
            font-family: "Helvetica";
            line-height: 1.6;
            font-stretch: expanded;
        }
        img.thumbnail {
            width: 64px;
            height: 64px;
        }
        img.error {
            width: 32px;
            height: 32px;
        }
        div.main_panel {
            max-width: 800px;
        }
        div.model_view {
            overflow: auto;
            max-width: 95%;
            max-height: 480px;
        }
        div.container {
            position: relative;
        }
        div.img_overlay {
            float: left;
            position: absolute;
            left: 0px;
            top: 0px;
            z-index: 1000;
            padding: 0px;
            font-weight: bold;
        }
        #img-model {
            max-width: 90%;
        }
        #div-tooltip {
            display: none;
            position: absolute;
        }
    </style>
    <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
    <script src="../{{ cgi_prefix }}static/scripts/graphics.min.js"></script>
    <script src="../{{ cgi_prefix }}static/scripts/upload.js"></script>
    <script>
        var face_services = function() {
            var canvas = 0;
            var EMOTIONS = ['angry', 'disgusted', 'fearful', 'happy', 'sad', 'surprised', 'neutral'];

            function round(n, precision=3) {
                var d = Math.pow(10, precision);
                return Math.round(n*d)/d;
            }

            function init() {
                if (canvas==0) {
                    canvas = acgraph.create('div-overlay');
                }
            }
            
            function render(request) {
                init();
                var services = request['services'];
                for (var j=0; j<services.length; j++) {
                    var service = services[j];
                    var results = service['results'];
                    var rects = results['rects'];
                    var predictions = results['predictions'];
                    var timing = results['timing'];
                    var ms_cnn_detect = 0;
                    for (var k=0; k<rects.length; k++) {
                        var rect = rects[k];
                        var p = predictions[k];
                        if (p[1] > 1.0) {
                            canvas.rect(rect[0], rect[1], rect[2], rect[3]).stroke('yellow', 2);
                        } else if (p[1] > p[0]) {
                            if (p[1] > 0.98) {
                                /*var t = acgraph.text(rect[0], rect[1]-15);
                                t.parent(canvas);
                                t.style({fontSize: '12px', color: 'lime'});
                                console.log(p[1]);
                                var confidence = Math.round(p[1]*100)/100;
                                t.text(confidence);*/
                                //canvas.rect(rect[0], rect[1], rect[2], rect[3]).stroke('lime', 2);
                                ms_cnn_detect++;
                            }
                        } else {
                            canvas.rect(rect[0], rect[1], rect[2], rect[3]).stroke('fuchsia', 2);
                        }
                    }
                    
                    // Display MTCNN rects and emotions
                    var mtcnn = results['mtcnn'];
                    var mtcnn_5p = results['mtcnn_5p'];
                    var emotions = results['emotions'];
                    for (var k=0; k<mtcnn.length; k++) {
                        var rect = mtcnn[k];
                        canvas.rect(rect[0], rect[1], rect[2], rect[3]).stroke('yellow', 2);

                        var points = mtcnn_5p[k];
                        for (var l=0; l<points.length; l++) {
                            var p = points[l];
                            //console.log(p);
                            //canvas.circle(p[0], p[1], 3).stroke('yellow', 1);
                        }

                        var e = emotions[k];
                        var emax = -1;
                        var emax_i = -1;
                        for (var l=0; l<EMOTIONS.length; l++) {
                            if (e[l] > emax) {
                                emax = e[l];
                                emax_i = l;
                            }
                        }
                        if (emax_i >= 0) {
                            var t = acgraph.text(rect[0], rect[1]-15);
                            t.parent(canvas);
                            t.style({fontSize: '12px', color: 'yellow'});
                            t.text(EMOTIONS[emax_i]);
                        }
                    }

                    if ('summary' in request) {
                        console.log('summary', request['summary']);
                        var summary = request['summary'];
                        console.log('timing', timing);
                        var t_server = summary['timing']['server_sent'] - summary['timing']['server_rcv'];
                        var t_total = summary['timing']['client_rcv'] - summary['timing']['client_sent'];
                        var t_transmission = t_total-t_server;
                        $('#div-summary').html(
                            'Faces detected: ' + rects.length + '<br/>' +
                            'Total response time: ' + round(t_total) + ' ms<br/>' +
                            'Transmission and client time: ' + round(t_transmission) + ' ms<br/>' +
                            'Total server time: ' + round(t_server) + ' ms<br/>' +
                            'Image processing time (server): ' + round(timing['preprocess']) + ' ms<br/>' +
                            'HOG+SVM detection time (server): ' + round(timing['detect']) + ' ms<br/>' +
                            'CNN multi-scale-detection time (server): ' + round(timing['cnn']) + ' ms<br/>' +
                            'MTCNN detection time (server): ' + round(timing['mtcnn']) + ' ms<br/>' +
                            'Emotion recognition time (server): ' + round(timing['emotion']) + ' ms<br/>' +
                            'Window count (positive/total): ' + ms_cnn_detect + '/' + timing['window_count'] + '<br/>'
                        );
                    }
                }
            }

            function preview(base64data) {
                init();
                $('#div-summary').html('Waiting for sever response...');
                canvas.remove();
                canvas = acgraph.create('div-overlay');
                var img = $('#img-sample');
                img.one('load', function() {
                    var img = $('#img-sample');
                    var div = $('#div-overlay')
                    $('#div-overlay').width(img.width()).height(img.height());
                });
                img.attr('src', base64data);
            }
            
            return {
                preview: preview,
                render: render,
            }
        }();

        $( document ).ready(function() {
            $('img.error').hover( function(e) {
                    var predict = $(this).attr('data-predict').split(",");
                    var img = $($('#div-classes .thumbnail').get(parseInt(predict[0])));
                    var src = img.attr("src");
                    $('#img-incorrect').attr("src", src);
                    var img = $($('#div-classes .thumbnail').get(parseInt(predict[1])));
                    var src = img.attr("src");
                    $('#img-correct').attr("src", src);
                    $('#div-tooltip').css({left:e.pageX+16, top:e.pageY+16}).show();
                }
            );

            $('#div-errors').mouseout( function(e) {
                    $('#div-tooltip').hide();
                }
            );

            $('#file_upload').change( function (e) {
                    mUploadManager.enqueueList({type: 'face', model: '12-net', endpoint: '../{{ cgi_prefix }}predict'}, $(this), function (status, response) {
                        if (status==mUploadManager.STATUS.Error) {
                            if (response.code==mUploadManager.ERROR.NotAFile) {
                                console.log('mUploadManager: Not a file');
                            } else if (response.code==mUploadManager.ERROR.InvalidType) {
                                console.log('mUploadManager: Unaccepted file type ' + response.fobj.type);
                            } else {
                                console.log('mUploadManager: Unhandled exception');
                                console.log(response);
                            }
                        } else if (status==mUploadManager.STATUS.Success) {
                            console.log('success', status, response);
                            face_services.render(response);
                        } else if (status==mUploadManager.STATUS.Sent) {
                            for (var i=0; i<response['requests'].length; i++) {
                                var request = response['requests'][i];
                                var base64img = 'data:image/jpeg;base64,' + request['media']['content'];
                                //console.log('sent', status, base64img);
                                face_services.preview(base64img);
                            }
                        } else {
                            console.log('unexpected', status, response);
                        }
                    });
                }
            );
        });
    </script>
    <body>
        <div id="div-tooltip" style="background-color: gray; padding: 8px;">
            predict / truth<br/>
            <img id="img-incorrect" class="thumbnail" src=""></img>
            <img id="img-correct" class="thumbnail" src=""></img>
        </div>
        <div class="main_panel">
            <h2>Arobot Computer Vision Services</h2>

            <h3>Face Detection and Classification</h3>
                <form id="form_upload" action="$action" enctype="multipart/form-data" method="post">
                    Upload files...
                    <input type="hidden" name="MAX_FILE_SIZE" value="30000" />
                    <input type="file" id="file_upload" name="upload" multiple="multiple"><br/>
                </form>

                <div style="width: 100%; display: table;">
                    <div id="div-results" class="container">
                        <div id="div-overlay" class="img_overlay" style="width: 400px; height: 400px;"></div>
                        <img id="img-sample"></img>
                    </div>
                    <div id="div-summary" style="display: table-cell; width: 360px; padding: 5px; vertical-align: top;"></div>
                </div>

            <h3>Model</h3>
                <ul>
                    <li>Name: 12-net
                    <li>Input: 12x12x3
                    <li>Activation: ReLU
                    <li>Batch normalization: No
                    <li>Initializer: Xavier
                    <li>Optimizer: Adam
                </ul>
            
            <h3>Training</h3>
                <ul>
                    <li>Training set: SoF + LFW
                    <li>Training size: 4,800 samples (50-50 faces and non-faces)
                    <li>Batch size: 240
                    <li>Steps: 2,000 per batch
                    <li>Learning rate: 0.0005
                </ul>
            
            <h3>Validation</h3>
                <ul>
                    <li>Validation set: WIKI
                    <li>Precision: 93.15%
                    <li>Recall: 89.5%
                </ul>
            
            <!--
            <h3>Method</h3>
            <ul>
                <li>Preprocess of images
                    <ul>
                        <li>Discard color (cv2.COLOR_RGB2GRAY)</li>
                        <li>Apply histogram equalization</li>
                        <li>Apply CLAHE, contrast limited adaptive histogram equalization</li>
                        <li>Flip to expand dataset</li>
                    </ul>
                </li>
                <li>Mutation
                    <ul>
                        <li>Rotation: ±15 degrees</li>
                        <li>Perspective transformation: ±20% of dimension on all 4 corners</li>
                    </ul>
                </li>
                <li>Training flow
                    <ul>
                        <li>Balanced data: Feed with same amount of samples in each class</li>
                        <li>Fine-tuning: Reduce learn rate (from 0.02 to 0.0001) and sample mutation intensity (from 1.0 to 0.75) after initial training</li>
                        <li>Use separate set for validation</li>
                        <li>Early stop if validation loss does not improve in 16 epochs</li>
                        <li>Save weights only when validation accuracy improves</li>
                    </ul>
                </li>
            </ul>
            -->

            <br/><br/><br/><br/>

        </div>
    </body>
</html>